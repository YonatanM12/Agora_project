{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pjtwunwiqwgG"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from sklearn.cluster import KMeans\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pD3X31Xym-6V"
   },
   "outputs": [],
   "source": [
    "# create dict for map all abbreviations/shortcuts of states into their full name\n",
    "\n",
    "res = requests.get(\"https://abbreviations.yourdictionary.com/articles/state-abbrev.html\")\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "lst = soup.find_all(\"tr\")[1:]\n",
    "states = [(s.contents[1].string,s.contents[0].string) for s in lst]\n",
    "states_dict = dict(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1-91cmoJtLNR"
   },
   "outputs": [],
   "source": [
    "# create dict for map all abbreviations/shortcuts of countries into their full name\n",
    "\n",
    "res2 = requests.get('https://www.iban.com/country-codes')\n",
    "soup = BeautifulSoup(res2.text, 'html.parser')\n",
    "lst = soup.find_all(\"tr\")[1:]\n",
    "country = [(c.contents[5].text,c.contents[1].text) for c in lst]\n",
    "country_dict = dict(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M9OEeNDRrgx6"
   },
   "outputs": [],
   "source": [
    "# combine the two dict into one\n",
    "\n",
    "dict_locations = {}\n",
    "for k,v in states_dict.items():\n",
    "  dict_locations[k] = v\n",
    "for k,v in country_dict.items():\n",
    "  dict_locations[k] = dict_locations.get(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop list for remove stop words from names of columns\n",
    "\n",
    "res3 = requests.get(\"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\")\n",
    "stop_list = [word for word in res3.text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(string):\n",
    "    \"\"\"replace all the abbreviations countries/states\n",
    "    to full name\"\"\"\n",
    "    \n",
    "   new_string = ''\n",
    "   try:\n",
    "    for n in string.split():\n",
    "        new_string += f'{dict_locations.get(n,n)} '\n",
    "    return new_string.strip()\n",
    "   except Exception:\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_loc(frame):\n",
    "    for col in frame.columns:\n",
    "        frame[col] = frame[col].apply(replacer)\n",
    "    return frame.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "#### Two steps:\n",
    "#### First, recognize which type the column belongs to: Account/Entity/General/Other,\n",
    "#### according to the column Name, tf-idf model.\n",
    "\n",
    "#### Second, recognize which field the column belongs to,\n",
    "#### according to the tf-idf model with also name entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "-6fmyWJFMNo5"
   },
   "outputs": [],
   "source": [
    "# Make counts per each column and each entity name in the data.\n",
    "# According to \"gensim name entity recognition\" model\n",
    "\n",
    "# ORG - Organization/Company\n",
    "# GPE - Geographic location\n",
    "# CARDINAL - Number, Also recognize Address\n",
    "# MONEY - Belongs to cash: Contribution, Distribution...\n",
    "# DATE - Dates\n",
    "# PERSON - Names of persons\n",
    "\n",
    "class counts:\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    fields = ['ORG','GPE','CARDINAL','MONEY','DATE','PERSON']\n",
    "    \n",
    "    def __init__(self,frame):\n",
    "        self.frame = frame\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.frequency = []\n",
    "    \n",
    "   \n",
    "    def counts_per_row(self,string):\n",
    "        doc = self.nlp(string)\n",
    "        counter = Counter([ent.label_ for ent in doc.ents])\n",
    "        return np.array([counter.get(k,0) for k in counts.fields])\n",
    "    \n",
    "    \n",
    "    def create_counts(self,frame):\n",
    "        counts_per_col = []\n",
    "        for col in frame.columns:\n",
    "            count = np.zeros(len(counts.fields))\n",
    "            for words in frame[col]:\n",
    "                try:\n",
    "                    count += self.counts_per_row(words)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            counts_per_col.append(count)\n",
    "        return counts_per_col\n",
    "    \n",
    "    # ***optional function***\n",
    "    def repr_freq(self,freq):\n",
    "        condition1 = np.where(freq[:,1] + freq[:,2] > 0.5)[0]\n",
    "        condition2 = np.where((freq[:,0] + freq[:,-1] > 0.8) & (freq[:,0] > freq[:,-1]))[0]\n",
    "        for i in range(len(freq)):\n",
    "            if i in condition1:\n",
    "                if freq[i,2] > 0.2:\n",
    "                    freq[i,2] = 0.8\n",
    "                    freq[i,1] = 0.2\n",
    "                freq[i,[0,-1]] = 0.\n",
    "            if i in condition2:\n",
    "                freq[i,0] = 0.8\n",
    "                freq[i,-1] = 0.2\n",
    "                freq[i,[1,2]] = 0.\n",
    "        return freq\n",
    "    \n",
    "    def counts_cols_percent(self):\n",
    "        freq = np.array(self.create_counts(self.frame))\n",
    "        summation = freq.sum(axis=1).reshape(-1,1)\n",
    "        summation[summation==0] = 1\n",
    "        freq = freq/summation\n",
    "#         freq = self.repr_freq(freq)\n",
    "        other = (freq > 0.4).sum(axis=1)\n",
    "        freq = np.concatenate((freq,((other==0).astype(np.int)).reshape(-1,1)),axis=1)\n",
    "        self.frequency = freq\n",
    "\n",
    "        \n",
    "    def plot_dist(self):\n",
    "        \n",
    "        dist = counts.pca.fit_transform(self.frequency)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        pos = 0.03\n",
    "        for value,cat in zip(dist, self.frame.columns):\n",
    "            pos *= -1\n",
    "            plt.scatter(value[0],value[1] , label=cat)\n",
    "            plt.annotate(cat, (value[0],value[1]), xytext=(value[0]-0.05, value[1]+pos), \n",
    "            arrowprops = dict(arrowstyle=\"wedge,tail_width=0.5\", alpha=0.1), fontsize=15)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def plot_frequency(self):\n",
    "        \n",
    "        for i,col in enumerate(self.frame.columns):\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.barplot(counts.fields+['OTHER'],self.frequency[i])\n",
    "            plt.title(col,size=20)\n",
    "            plt.ylabel('count in %',size=15)\n",
    "            plt.xlabel('entities',size=15)\n",
    "            plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the documents from files, which contains the words for the tf-idf,\n",
    "### and makes lists of the different groups of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acount_Names = []\n",
    "Entity_Names = []\n",
    "General_Names = []\n",
    "docs = [Acount_Names,Entity_Names,General_Names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG = []\n",
    "GPE = []\n",
    "CARD = []\n",
    "MONEY = []\n",
    "DATE = []\n",
    "PERSON = []\n",
    "OTHER = []\n",
    "ent_names = [ORG,GPE,CARD,MONEY,DATE,PERSON,OTHER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = ['Account_Names.txt','Entity_Names.txt','General_Names.txt']\n",
    "for file,L in zip(dir,docs):\n",
    "    with open(f'{file}') as f:\n",
    "        lst = f.readlines()\n",
    "        for l in lst:\n",
    "            L.append(l.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = ['ORG.txt','GPE.txt','CARD.txt','MONEY.txt','DATE.txt','PERSON.txt','OTHER.txt']\n",
    "for file,L in zip(dir,ent_names):\n",
    "    with open(f'{file}') as f:\n",
    "        lst = f.readlines()\n",
    "        for l in lst:\n",
    "            L.append(l.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(lst):\n",
    "    \"\"\"remove punctuations from the column names\"\"\"\n",
    "    new_list = []\n",
    "    for s in lst:\n",
    "        for word in re.split('_|=|-|:|,|/|[(]|[)]| ',s.lower()):\n",
    "            if word not in stop_list:\n",
    "                new_list.append(word)\n",
    "    return new_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "0ndFrbI-IfjJ"
   },
   "outputs": [],
   "source": [
    "# Create similarity index of the types of the columns(Account,Entity,General)\n",
    "# in order to check the similarity between each column name and the types\n",
    "\n",
    "types_names_docs = [strip_punc(doc) for doc in docs]\n",
    "types_dictionary = gensim.corpora.Dictionary(types_names_docs)\n",
    "types_corpus = [types_dictionary.doc2bow(gen_doc) for gen_doc in types_names_docs]\n",
    "types_tf_idf = gensim.models.TfidfModel(types_corpus)\n",
    "sims = gensim.similarities.Similarity('/content',types_tf_idf[types_corpus],\n",
    "                                        num_features=len(types_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "tXDiphMkpv3h"
   },
   "outputs": [],
   "source": [
    "# Create similarity index of the fields of the columns('ORG','GPE','CARDINAL','MONEY','DATE','PERSON')\n",
    "# in order to check the similarity between each column name and the field\n",
    "\n",
    "entities = [strip_punc(doc) for doc in ent_names]\n",
    "ent_dictionary = gensim.corpora.Dictionary(entities)\n",
    "ent_corpus = [ent_dictionary.doc2bow(gen_doc) for gen_doc in entities]\n",
    "tf_idf_ent = gensim.models.TfidfModel(ent_corpus)\n",
    "ent_sims = gensim.similarities.Similarity('/content',tf_idf_ent[ent_corpus],\n",
    "                                        num_features=len(ent_dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_type(frame):\n",
    "    '''return the indexes of the columns per each type'''\n",
    "    \n",
    "    corpus = [types_dictionary.doc2bow(re.split('_|=|-|:|,| ',col.lower())) for col in frame.columns]\n",
    "    groups = sims[types_tf_idf[corpus]]\n",
    "    other = ((groups.sum(axis=1)==0).astype(int)).reshape(-1,1)\n",
    "    cols_type = (np.concatenate((groups,other),axis=1)) + 1\n",
    "    account_cols = np.where(np.argmax(cols_type,axis=1)==0)[0]\n",
    "    entity_cols = np.where(np.argmax(cols_type,axis=1)==1)[0]\n",
    "    general_cols = np.where(np.argmax(cols_type,axis=1)==2)[0]\n",
    "    other_cols = np.where(np.argmax(cols_type,axis=1)==3)[0]\n",
    "    return account_cols,entity_cols,general_cols,other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(frame):\n",
    "    '''return the indexes of the columns per each field'''\n",
    "    \n",
    "    corpus = [ent_dictionary.doc2bow(re.split('_|=|-|:|,| ',col.lower())) for col in frame.columns]\n",
    "    groups = ent_sims[tf_idf_ent[corpus]] + 1\n",
    "    count = counts(frame)\n",
    "    count.counts_cols_percent()\n",
    "    frequencies = count.frequency + 1\n",
    "    entity_freq = (frequencies)*(9**(groups))\n",
    "    ORG_cols = np.where(entity_freq.argmax(axis=1)==0)[0]\n",
    "    GPE_cols = np.where(entity_freq.argmax(axis=1)==1)[0]\n",
    "    CARDINAL_cols = np.where(entity_freq.argmax(axis=1)==2)[0]\n",
    "    MONEY_cols = np.where(entity_freq.argmax(axis=1)==3)[0]\n",
    "    DATE_cols = np.where(entity_freq.argmax(axis=1)==4)[0]\n",
    "    PERSON_cols = np.where(entity_freq.argmax(axis=1)==5)[0]\n",
    "    OTHER_cols = np.where(entity_freq.argmax(axis=1)==6)[0]\n",
    "    \n",
    "    return ORG_cols,GPE_cols,CARDINAL_cols,MONEY_cols,DATE_cols,PERSON_cols,OTHER_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict(frame):\n",
    "    '''print for each column in the data the predicted\n",
    "        type and the field'''\n",
    "    \n",
    "    types = [get_column_type(frame)]\n",
    "    fields = [get_entities(frame)]\n",
    "    \n",
    "    types_names = ['Account','Entity','General','OTHER']\n",
    "    fields_names = ['ORG', 'GPE', 'CARDINAL', 'MONEY', 'DATE', 'PERSON', 'OTHER']\n",
    "    \n",
    "    idx_type = {}\n",
    "    idx_field = {}\n",
    "    \n",
    "    for name,group in zip(types_names,types[0]):\n",
    "        for id in group:\n",
    "            idx_type[id] = name\n",
    "    for name,group in zip(fields_names,fields[0]):\n",
    "        for id in group:\n",
    "            idx_field[id] = name\n",
    "            \n",
    "    for i,col in enumerate(frame.columns):\n",
    "        print(f'column name: {col},  predict: {idx_type[i], idx_field[i]}\\n')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Agora_Alpha",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
